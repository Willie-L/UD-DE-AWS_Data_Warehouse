# Udacity Data Engineering with AWS Nanodegree Project: Data Warehouse

---

## Sparkify S3 to Redshift ETL

A music streaming startup, Sparkify, has grown their user base and song database and want to move their processes and data onto the cloud. Their data resides in S3, in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app.

As their data engineer, I'm tasked with building an ETL pipeline that extracts their data from S3, stages them in Redshift, and transforms data into a set of dimensional tables for their analytics team to continue finding insights into what songs their users are listening to.

## Project Description

In this project, I'll apply what I've learned on data warehouses and AWS to build an ETL pipeline for a database hosted on Redshift. To complete the project, I'll load data from S3 to staging tables on Redshift and execute SQL statements that create the analytics tables from these staging tables.

## Requirements

- [Python](https://www.python.org/downloads/) v3.8 or higher
  - The module 'psycopg2' is used in the scripts. To install run the following command:

  ```bash
  pip install psycopg2
  ```

- [Amazon Redshift](https://docs.aws.amazon.com/redshift/)

## Project Datasets

I'll be working with two datasets stored in S3:

- Song data: s3://udacity-dend/song_data
- Log data: s3://udacity-dend/log_data
- Log metadata: s3://udacity-dend/log_json_path.json

### Song Dataset

The first dataset is a subset of real data from the Million Song Dataset. Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID. For example, here are file paths to two files in this dataset.

```text
song_data/A/B/C/TRABCEI128F424C983.json
song_data/A/A/B/TRAABJL12903CDCF1A.json
```

Below is an example single song file.

```json
 {"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}
```

### Log Dataset

The second dataset consists of log files in JSON format generated by an event simulator based on the songs in the dataset above. These simulate app activity logs from an imaginary music streaming app based on configuration settings.

The log files in the dataset I'll be working with are partitioned by year and month. For example, here are file paths to two files in this dataset.

```text
log_data/2018/11/2018-11-12-events.json
log_data/2018/11/2018-11-13-events.json
```

And below is a visual example of a log file.
![log data](image location)

## Database Schema

### Staging tables

![staging_tables](https://github.com/Willie-L/UD-DE-AWS_Data_Warehouse/tree/master/images/staging_tables.png)

### Fact and Dimension Tables

![fact_and_dim_tables](https://github.com/Willie-L/UD-DE-AWS_Data_Warehouse/tree/master/images/fact_and_dim_tables.png)

**factSongplay:**

- ***distkey***: `song_id` is the best choice for the distribution key in my opinion as the vast majority of queries will be based on distinct song plays.
  - This key is also identified in the **dimSong** table.
- ***sortkey***: I chose `start_time` as the sort key as I believe most analytical queries performed by the team will heavily depend on timeframe such as what are the top 5 most listened to songs last summer. This way, events can be easily filtered out to the desired timeframe and retrieved with greater speed and efficiency.
  - This sortkey is also identified in the **dimTime** table.

## How to Execute

The following files are included in the repository:

- `sql_queries.py`: contains all the necessary SQL queries used in the python scripts.
- `create_tables.py`: creates the staging tables as well as the fact and dimension tables that will be used by the analytics team.
- `etl.py`: executes the ETL pipeline that will extract the JSON data from S3, insert it into the staging tables, then transform the data into the required set of analytics tables.
- `dwh_.cfg`: an example configuration file that would hold such items like the credentials needed to connect and manipulate the database. <p style="color:red">This type of file should never be shared if it does contain this type of sensitive information.</p>

Each python script is ran in the terminal using the following command:

```bash
python3 [python file path]
```

Begin by running `create_tables.py` then finish with `etl.py`.

## Project Summary

I began the project by considering the schema design. I created a visual representation of the staging tables then moved on to the fact and dimension tables. A good amount of time was spent deciding which data types best represent the data and what kind of questions would be the analytics team want to ask. Once that was done, I created the SQL queries. I then created a new Redshift cluster and added its information to a `dwh.cfg` file. The `create_tables.py` script was next where I created the functions that would execute the `CREATE` queries I wrote in the `sql_queries.py` file. The script was tested and errors corrected prior to writing the `etl.py` script. In it, I wrote two functions to load the staging table data and transfrom the stagiing data into the five fact and dimension tables. I used psycopg2's error handling in both python scripts, but for the `etl.py` script I added some code to check the perfomance of the COPY and INSERT statements that moved the data into the staging tables then subsequently transformed into the fact and dimension tables. I ran the script and checked for any errors prior to running a few SQL queries on the new tables to confirm my expected results using Redshift's Query Editor.
